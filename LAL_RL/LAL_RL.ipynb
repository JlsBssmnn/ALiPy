{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for LAL_RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# from tensorflow.keras import Sequential, layers\n",
    "# from tensorflow.keras.optimizers import Adam\n",
    "# from keras.utils.vis_utils import plot_model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import random\n",
    "from collections import namedtuple\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, candidate_size, bias_initialization=None):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(candidate_size, 10)\n",
    "        self.fc2 = nn.Linear(13, 5)\n",
    "        self.fc3 = nn.Linear(5, 1)\n",
    "        \n",
    "        if bias_initialization is not None:\n",
    "            self.fc3.bias = torch.nn.Parameter(torch.tensor(bias_initialization, dtype=torch.float))\n",
    "        \n",
    "    def forward(self, t):\n",
    "        state = t[:-3]\n",
    "        action = t[-3:]\n",
    "        t = torch.sigmoid(self.fc1(state))\n",
    "        t = torch.cat((t,action))\n",
    "        t = torch.sigmoid(self.fc2(t))\n",
    "        t = self.fc3(t)\n",
    "        return t\n",
    "    \n",
    "\n",
    "class DQN:\n",
    "    def __init__(self, candidate_size, learning_rate=0.0001, bias_initialization=None, device=None):\n",
    "        self.net = Net(candidate_size, bias_initialization).to(device)\n",
    "        self.loss = nn.MSELoss()\n",
    "        self.optimizer = optim.Adam(self.net.parameters(), lr=learning_rate)\n",
    "        self.batch_size = 32\n",
    "        \n",
    "    def fit(self, X, y, epochs=1, device=None):\n",
    "        for _ in range(epochs):\n",
    "            out = self.net(X)\n",
    "            loss = self.loss(out, y)\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "        \n",
    "    def predict(self, X):\n",
    "        return self.net(X).view(X.size(0))\n",
    "\n",
    "    def get_weights(self):\n",
    "        return self.net.state_dict()\n",
    "    \n",
    "    def set_weights(self, weights):\n",
    "        self.net.load_state_dict(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    \"\"\"\n",
    "    this class creates states and actions from the given parameters\n",
    "    acording to section 3.2.1 and 3.2.2 of the paper\n",
    "    \"\"\"\n",
    "    def __init__(self, distance, measure=\"zero_probability\", device=None):\n",
    "        self.measure = measure\n",
    "        self.distance = distance\n",
    "        self.device = device\n",
    "    \n",
    "    def create_state(self, data, model):\n",
    "        if self.measure == \"zero_probability\":\n",
    "            prediction = model.predict_proba(data)\n",
    "            state_vec = prediction[:,0]\n",
    "            state_vec = np.sort(state_vec)\n",
    "            return torch.tensor(state_vec, device=self.device)\n",
    "        else:\n",
    "            raise ValueError(f\"provided measure '{self.measure}' is not supported\")\n",
    "        \n",
    "    def create_action(self, sample, model, labeled_set, unlabeled_set):\n",
    "        if self.measure==\"zero_probability\":\n",
    "            score = model.predict_proba([sample])[0][0]\n",
    "        else:\n",
    "            raise ValueError(f\"provided measure '{self.measure}' is not supported\")\n",
    "        \n",
    "        #calculate distance to labeled data set\n",
    "        dis_lab = sum([self.distance(sample, x) for x in labled_set]) / len(labeled_set)\n",
    "        \n",
    "        #calculate distance to unlabeled data set\n",
    "        dis_unlab = sum([self.distance(sample, x) for x in unlabled_set]) / len(unlabeled_set)\n",
    "        \n",
    "        return torch.Tensor([score, dis_lab, dis_unlab], device=self.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, q_network, discount=0.999):\n",
    "        self.current_state = None\n",
    "        self.q_network = q_network\n",
    "        self.target_network = copy.deepcopy(q_network)\n",
    "        self.iteration = 0\n",
    "        self.discount = discount\n",
    "        \n",
    "        # TODO: support own parameters for epsilon greedy strategy\n",
    "        self.strategy = Epsilon_Greedy_Strat(1, 0, 0.001)\n",
    "        \n",
    "    def set_starting_state(self, state):\n",
    "        self.current_state = state\n",
    "        \n",
    "    def reset(self):\n",
    "        self.iteration = 0\n",
    "        \n",
    "    def select_action(self, possible_actions):\n",
    "        self.iteration += 1\n",
    "        if self.strategy.explore(self.iteration):\n",
    "            # explore aka pick a random action\n",
    "            return random.randrange(len(possible_actions))\n",
    "        else:\n",
    "            # pick the best action according to the current policy\n",
    "            input_values = torch.cat((self.current_state[None,:].repeat(len(possible_actions,1), \n",
    "                                                                        possible_actions), dim=1))\n",
    "            q_values = self.q_network.predict(input_values)\n",
    "            return torch.argmax(q_values).item()\n",
    "        \n",
    "    def train(self, experiences):\n",
    "        X = np.array([np.concatenate((x[0],x[1])) for x in experiences])\n",
    "        y = np.array([x[2] for x in experiences])\n",
    "        \n",
    "        # add gamma*maxQ(s_t+1, a_t+1) if the state is not an end state\n",
    "        y = np.array([y[i] + self.discount * self.get_max_q_value_target(experiences[i][3], experiences[i][5])\n",
    "                     if not experiences[i][4] else y[i] for i in range(len(y))])\n",
    "        \n",
    "        self.q_network.fit(X,y)\n",
    "        \n",
    "    def get_max_q_value_target(self, state, possible_actions):\n",
    "        input_values = np.array([np.concatenate((state, x)) for x in possible_actions])\n",
    "        q_values = self.target_network.predict(input_values)\n",
    "        return max([x[0] for x in q_values])\n",
    "    \n",
    "    def update_target_network(self):\n",
    "        self.target_network.set_weights(self.q_network.get_weights())\n",
    "            \n",
    "        \n",
    "class Epsilon_Greedy_Strat:\n",
    "    def __init__(self, start, _min, decay):\n",
    "        self.start = start\n",
    "        self._min = _min\n",
    "        self.decay = decay\n",
    "        \n",
    "    def explore(self, interration):\n",
    "        \"\"\"\n",
    "        returns true if the agent should explore and false if the agent should exploit\n",
    "        \"\"\"\n",
    "        return random.random() < max(self.start - self.decay*interration, self._min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Experience = namedtuple(\"Experience\", [\"state\", \"action\", \"reward\", \"new_state\", \"is_endstate\", \"possible_actions\"])\n",
    "\n",
    "class ReplayMemory:\n",
    "    \"\"\"\n",
    "    stores the experience of agent of the form:\n",
    "        (state, taken action, reward, new state,\n",
    "        if new state is end state, possible actions in new state)\n",
    "    \"\"\"\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.index = 0\n",
    "        \n",
    "    def push(self, elem):\n",
    "        if len(memory) < self.capacity:\n",
    "            self.memory.append(copy.deepcopy(elem))\n",
    "        else:\n",
    "            self.memory[self.index] = copy.deepcopy(elem)\n",
    "        \n",
    "        self.index = (self.index + 1) % self.capacity\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def can_provide(self, size):\n",
    "        return len(memory) >= size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LAL_RL:\n",
    "    # TODO: support multiple Datasets, own replay memory size, other target quality besides accuracy\n",
    "    def __init__(self, X, y, eval_ratio, candidate_size, initially_labeled, model, target_quality, distance,\n",
    "                measure=\"zero_probability\", batch_size=32, update_rate=100):\n",
    "        all_data = np.column_stack((X,y))\n",
    "        \n",
    "        self.eval_data_ind = round(len(all_data)*eval_ratio)\n",
    "        self.unlab_data_ind = len(all_data) - initially_labeled\n",
    "        if self.unlab_data_ind <= self.eval_data_ind:\n",
    "            raise ValueError(\"Not enough samples to split the data properly\")\n",
    "        \n",
    "        self.split_data(all_data)\n",
    "\n",
    "        self.environment = Environment(distance, measure)\n",
    "        self.agent = Agent(DQN(candidate_size))\n",
    "        self.model = model\n",
    "        self.candidate_size = candidate_size\n",
    "        \n",
    "        # find out the accuracy when all data is labeled\n",
    "        tmp_model = copy.deepcopy(model)\n",
    "        tmp_model.fit(X[self.eval_data_ind:], y[self.eval_data_ind:])\n",
    "        pred = tmp_model.predict(torch.tensor(self.eval_X, device=device))\n",
    "        pred = pred == torch.tensor(self.eval_y, device=device)\n",
    "        accuracy = torch.sum(pred) / len(pred)\n",
    "        \n",
    "        # set target quality to be a portion of the accuracy when all data is labeled\n",
    "        self.target_quality = target_quality * accuracy\n",
    "        \n",
    "        self.replay_memory = ReplayMemory(10000)\n",
    "        self.batch_size = batch_size\n",
    "        self.update_rate = update_rate\n",
    "        \n",
    "    def learn_q_function(self,episodes):\n",
    "        for i in episodes:\n",
    "            self.learn_episode()\n",
    "            \n",
    "            if i % self.update_rate == 0:\n",
    "                # copy the weight of the q_network to the target network\n",
    "                self.agent.update_target_network()\n",
    "                \n",
    "            self.split_data()\n",
    "            self.agent.reset()\n",
    "            \n",
    "    def learn_episode(self):\n",
    "        model = copy.deepcopy(self.model)\n",
    "        done = False\n",
    "        X,y = self.get_X_y(self.lab_data)\n",
    "        model.fit(X,y)\n",
    "        \n",
    "        V, idx = self.random_2D_sample(self.unlab_data, self.candidate_size)\n",
    "        V_X, V_y = self.get_X_y(V)\n",
    "        state = self.environment.create_state(V_X, model)\n",
    "        self.agent.set_starting_state(state)\n",
    "        \n",
    "        possible_actions = [self.environment.create_action(x, model, self.lab_data, self.unlab_data)\n",
    "                               for x in V_X]\n",
    "        \n",
    "        while not done:\n",
    "            # choose an action\n",
    "            action_idx = self.agent.select_action(possible_actions)\n",
    "            action = possible_actions[action_idx]\n",
    "            \n",
    "            # update labeled and unlabeled dataset\n",
    "            self.label_sample(idx[action_idx])\n",
    "            \n",
    "            # retrain the model\n",
    "            X,y = self.get_X_y(self.lab_data)\n",
    "            model.fit(X,y)\n",
    "            \n",
    "            # create the next state\n",
    "            V, idx = self.random_2D_sample(self.unlab_data, self.candidate_size)\n",
    "            V_X, V_y = self.get_X_y(V)\n",
    "            new_state = self.environment.create_state(V_X, model)\n",
    "            \n",
    "            # update possible actions\n",
    "            possible_actions = [self.environment.create_action(x, model, self.lab_data, self.unlab_data)\n",
    "                               for x in V_X]\n",
    "            \n",
    "            # calculate accuracy for the newly trained model\n",
    "            X_test, y_test = self.get_X_y(self.eval_data)\n",
    "            pred = model.predict(X_test)\n",
    "            equality = tf.math.equal(pred, y_test)\n",
    "            accuracy = tf.math.reduce_mean(tf.cast(equality, tf.float32))\n",
    "            \n",
    "            # test if target quality has been reached or if there is not enough unlabeled data\n",
    "            done = accuracy >= self.target_quality\n",
    "            done |= len(self.unlab_data) <= self.candidate_size\n",
    "            \n",
    "            # safe the experience\n",
    "            self.replay_memory.push((state, action, -1, new_state, done, possible_actions))\n",
    "            \n",
    "            # update state\n",
    "            state = new_state\n",
    "            self.agent.current_state = state\n",
    "            \n",
    "            # sample a batch (if possible) and train the DQN\n",
    "            # TODO: consider the TD-Error when sampling\n",
    "            if self.replay_memory.can_provide(self.batch_size):\n",
    "                self.agent.train(self.replay_memory.sample(self.batch_size))\n",
    "            \n",
    "    \n",
    "    def random_2D_sample(array, size):\n",
    "        \"\"\"\n",
    "        sample a total of 'size' random 1-D arrays out of a 2-D array\n",
    "        \"\"\"\n",
    "        if size > len(array):\n",
    "            raise ValueError(\"Tried to sample more data than existed\")\n",
    "        idx = np.random.randint(len(array), size=size)\n",
    "        return array[idx,:], idx\n",
    "    \n",
    "    def get_X_y(self, data):\n",
    "        \"\"\"\n",
    "        if X and y are united in data, return X and y seperated\n",
    "        \"\"\"\n",
    "        n_features = len(data[0]) - 1\n",
    "        return np.delete(data, n_features, 1), np.delete(data, range(n_features), 1)\n",
    "    \n",
    "    def label_sample(self, idx):\n",
    "        \"\"\"\n",
    "        moves the sample self.unlab_data[idx] to self.lab_data\n",
    "        \"\"\"\n",
    "        sample = self.unlab_data[idx]\n",
    "        self.unlab_data = np.delete(self.unlab_data, idx, 0)\n",
    "        self.lab_data = np.concatenate((self.lab_data, np.expand_dims(sample, axis=0)))\n",
    "        \n",
    "    def split_data(self, all_data):\n",
    "        \"\"\"\n",
    "        distributes the data among evaluation, unlabeled and labeled data\n",
    "        \"\"\"\n",
    "        np.random.shuffle(all_data)\n",
    "        self.eval_X, self.eval_y = all_data[:self.eval_data_ind, :-1], all_data[:self.eval_data_ind, -1]\n",
    "        self.unlab_X, self.unlab_y = all_data[self.eval_data_ind:self.unlab_data_ind, :-1],\\\n",
    "                                     all_data[self.eval_data_ind:self.unlab_data_ind, -1]\n",
    "        self.lab_X, self.lab_y = all_data[self.unlab_data_ind:, :-1], all_data[self.unlab_data_ind:, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Only for testing purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 1 1]\n",
      " [2 2 2 2 2]\n",
      " [3 3 3 3 3]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([11, 22, 33])"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array([[1,1,1,1,1],[2,2,2,2,2],[3,3,3,3,3]])\n",
    "y = np.array([11,22,33])\n",
    "all_data = np.column_stack((X,y))\n",
    "newX = all_data[:,:-1]\n",
    "newy = all_data[:,-1]\n",
    "print(newX)\n",
    "newy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "qnet = DQN(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    class T:\n",
    "        def __init__(self,t):\n",
    "            self.t = t\n",
    "            \n",
    "    return T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "theclass = test()\n",
    "testi = theclass(4)\n",
    "print(testi.t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001E66CEE6940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[[-0.5097586 ]\n",
      " [-0.07278678]\n",
      " [-0.1229156 ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-0.07278678"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network = Sequential(\n",
    "            [\n",
    "                layers.Dense(5, input_shape=(3,), activation=\"sigmoid\", name=\"layer1\"),\n",
    "#                 layers.Dense(80, activation=\"sigmoid\", name=\"layer2\"),\n",
    "                layers.Dense(1, name=\"layer3\"),\n",
    "            ]\n",
    "        )\n",
    "# print(network.get_weights())\n",
    "# plot_model(network, show_shapes=True, show_layer_names=True)\n",
    "pred = network.predict(np.array([np.array([1,2,3]),np.array([3,2,1]),np.array([5,2,4])]))\n",
    "print(pred)\n",
    "max([x[0] for x in pred])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
