{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for LAL_RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential, layers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.utils.vis_utils import plot_model\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN:\n",
    "    def __init__(self, candidate_size, learning_rate=0.0001):\n",
    "        self.q_network = Sequential(\n",
    "            [\n",
    "                layers.Dense(80, input_shape=(candidate_size+3,), activation=\"sigmoid\", name=\"layer1\"),\n",
    "                layers.Dense(80, activation=\"sigmoid\", name=\"layer2\"),\n",
    "                layers.Dense(1, name=\"layer3\"),\n",
    "            ]\n",
    "        )\n",
    "        self.q_network.compile(optimizer=Adam(learning_rate=learning_rate), loss='mse', metrics=['accuracy'])\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.q_network.fit(X,y)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        return self.q_network.predict(X)\n",
    "\n",
    "    def get_weights(self):\n",
    "        return self.q_network.get_weights()\n",
    "    \n",
    "    def set_weights(self, weights):\n",
    "        self.q_network.set_weights(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    \"\"\"\n",
    "    this class creates states and actions from the given parameters\n",
    "    acording to section 3.2.1 and 3.2.2 of the paper\n",
    "    \"\"\"\n",
    "    def __init__(self, distance, measure=\"zero_probability\"):\n",
    "        self.measure = measure\n",
    "        self.distance = distance\n",
    "    \n",
    "    def create_state(self, data, model):\n",
    "        if self.measure==\"zero_probability\":\n",
    "            prediction = model.predict_proba(data)\n",
    "            state_vec = np.array([x[0] for x in prediction])\n",
    "            return np.sort(state_vec)\n",
    "        else:\n",
    "            raise ValueError(f\"provided measure '{self.measure}' is not supported\")\n",
    "        \n",
    "    def create_action(self, sample, model, labeled_set, unlabeled_set):\n",
    "        if self.measure==\"zero_probability\":\n",
    "            score = model.predict_proba([sample])[0][0]\n",
    "        else:\n",
    "            raise ValueError(f\"provided measure '{self.measure}' is not supported\")\n",
    "        \n",
    "        #calculate distance to labeled data set\n",
    "        dis_lab = sum([self.distance(sample, x) for x in labled_set]) / len(labeled_set)\n",
    "        \n",
    "        #calculate distance to unlabeled data set\n",
    "        dis_unlab = sum([self.distance(sample, x) for x in unlabled_set]) / len(unlabeled_set)\n",
    "        \n",
    "        return np.array([score, dis_lab, dis_unlab])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, q_network, discount=0.999):\n",
    "        self.current_state = None\n",
    "        self.q_network = q_network\n",
    "        self.target_network = copy.deepcopy(q_network)\n",
    "        self.iteration = 0\n",
    "        self.discount = discount\n",
    "        \n",
    "        # TODO: support own parameters for epsilon greedy strategy\n",
    "        self.strategy = Epsilon_Greedy_Strat(1, 0, 0.001)\n",
    "        \n",
    "    def set_starting_state(self, state):\n",
    "        self.current_state = state\n",
    "        \n",
    "    def reset(self):\n",
    "        self.iteration = 0\n",
    "        \n",
    "    def select_action(self, possible_actions):\n",
    "        self.iteration += 1\n",
    "        if self.strategy.explore(self.iteration):\n",
    "            # explore aka pick a random action\n",
    "            return random.randrange(len(possible_actions))\n",
    "        else:\n",
    "            # pick the best action according to the current policy\n",
    "            input_values = np.array([np.concatenate((self.current_state, x)) for x in possible_actions])\n",
    "            q_values = self.q_network.predict(input_values)\n",
    "            return np.argmax(q_values, axis=0)[0]\n",
    "        \n",
    "    def train(self, experiences):\n",
    "        X = np.array([np.concatenate((x[0],x[1])) for x in experiences])\n",
    "        y = np.array([x[2] for x in experiences])\n",
    "        \n",
    "        # add gamma*maxQ(s_t+1, a_t+1) if the state is not an end state\n",
    "        y = np.array([y[i] + self.discount * self.get_max_q_value_target(experiences[i][3], experiences[i][5])\n",
    "                     if not experiences[i][4] else y[i] for i in range(len(y))])\n",
    "        \n",
    "        self.q_network.fit(X,y)\n",
    "        \n",
    "    def get_max_q_value_target(self, state, possible_actions):\n",
    "        input_values = np.array([np.concatenate((state, x)) for x in possible_actions])\n",
    "        q_values = self.target_network.predict(input_values)\n",
    "        return max([x[0] for x in q_values])\n",
    "    \n",
    "    def update_target_network(self):\n",
    "        self.target_network.set_weights(self.q_network.get_weights())\n",
    "            \n",
    "        \n",
    "class Epsilon_Greedy_Strat:\n",
    "    def __init__(self, start, _min, decay):\n",
    "        self.start = start\n",
    "        self._min = _min\n",
    "        self.decay = decay\n",
    "        \n",
    "    def explore(self, interration):\n",
    "        \"\"\"\n",
    "        returns true if the agent should explore and false if the agent should exploit\n",
    "        \"\"\"\n",
    "        return random.random() < max(self.start - self.decay*interration, self._min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory:\n",
    "    \"\"\"\n",
    "    stores the experience of agent of the form:\n",
    "        (state, taken action, reward, new state,\n",
    "        if new state is end state, possible actions in new state)\n",
    "    \"\"\"\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.index = 0\n",
    "        \n",
    "    def push(self, elem):\n",
    "        if len(memory) < self.capacity:\n",
    "            self.memory.append(copy.deepcopy(elem))\n",
    "        else:\n",
    "            self.memory[self.index] = copy.deepcopy(elem)\n",
    "        \n",
    "        self.index = (self.index + 1) % self.capacity\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def can_provide(self, size):\n",
    "        return len(memory) >= size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LAL_RL:\n",
    "    # TODO: support multiple Datasets, own replay memory size, other target quality besides accuracy\n",
    "    def __init__(self, X, y, eval_ratio, candidate_size, initially_labeled, model, target_quality, distance,\n",
    "                measure=\"zero_probability\", batch_size=32, update_rate=100):\n",
    "        self.all_data = np.column_stack((X,y))\n",
    "        \n",
    "        self.eval_data_ind = round(len(all_data)*eval_ratio)\n",
    "        self.unlab_data_ind = len(all_data) - initially_labeled\n",
    "        if self.unlab_data_ind <= self.eval_data_ind:\n",
    "            raise ValueError(\"Not enough samples to split the data properly\")\n",
    "        \n",
    "        self.split_data()\n",
    "\n",
    "        self.environment = Environment(distance, measure)\n",
    "        self.agent = Agent(DQN(candidate_size))\n",
    "        self.model = model\n",
    "        self.candidate_size = candidate_size\n",
    "        \n",
    "        # find out the accuracy when all data is labeled\n",
    "        tmp_model = copy.deepcopy(model)\n",
    "        tmp_model.fit(X[eval_data_ind:], y[eval_data_ind:])\n",
    "        pred = tmp_model.predict(X[:eval_data_ind])\n",
    "        equality = tf.math.equal(pred, y[:eval_data_ind])\n",
    "        accuracy = tf.math.reduce_mean(tf.cast(equality, tf.float32))\n",
    "        \n",
    "        # set target quality to be a portion of the accuracy when all data is labeled\n",
    "        self.target_quality = target_quality * accuracy\n",
    "        \n",
    "        self.replay_memory = ReplayMemory(10000)\n",
    "        self.batch_size = batch_size\n",
    "        self.update_rate = update_rate\n",
    "        \n",
    "    def learn_q_function(self,episodes):\n",
    "        for i in episodes:\n",
    "            self.learn_episode()\n",
    "            \n",
    "            if i % self.update_rate == 0:\n",
    "                # copy the weight of the q_network to the target network\n",
    "                self.agent.update_target_network()\n",
    "                \n",
    "            self.split_data()\n",
    "            self.agent.reset()\n",
    "            \n",
    "    def learn_episode(self):\n",
    "        model = copy.deepcopy(self.model)\n",
    "        done = False\n",
    "        X,y = self.get_X_y(self.lab_data)\n",
    "        model.fit(X,y)\n",
    "        \n",
    "        V, idx = self.random_2D_sample(self.unlab_data, self.candidate_size)\n",
    "        V_X, V_y = self.get_X_y(V)\n",
    "        state = self.environment.create_state(V_X, model)\n",
    "        self.agent.set_starting_state(state)\n",
    "        \n",
    "        possible_actions = [self.environment.create_action(x, model, self.lab_data, self.unlab_data)\n",
    "                               for x in V_X]\n",
    "        \n",
    "        while not done:\n",
    "            # choose an action\n",
    "            action_idx = self.agent.select_action(possible_actions)\n",
    "            action = possible_actions[action_idx]\n",
    "            \n",
    "            # update labeled and unlabeled dataset\n",
    "            self.label_sample(idx[action_idx])\n",
    "            \n",
    "            # retrain the model\n",
    "            X,y = self.get_X_y(self.lab_data)\n",
    "            model.fit(X,y)\n",
    "            \n",
    "            # create the next state\n",
    "            V, idx = self.random_2D_sample(self.unlab_data, self.candidate_size)\n",
    "            V_X, V_y = self.get_X_y(V)\n",
    "            new_state = self.environment.create_state(V_X, model)\n",
    "            \n",
    "            # update possible actions\n",
    "            possible_actions = [self.environment.create_action(x, model, self.lab_data, self.unlab_data)\n",
    "                               for x in V_X]\n",
    "            \n",
    "            # calculate accuracy for the newly trained model\n",
    "            X_test, y_test = self.get_X_y(self.eval_data)\n",
    "            pred = model.predict(X_test)\n",
    "            equality = tf.math.equal(pred, y_test)\n",
    "            accuracy = tf.math.reduce_mean(tf.cast(equality, tf.float32))\n",
    "            \n",
    "            # test if target quality has been reached or if there is not enough unlabeled data\n",
    "            done = accuracy >= self.target_quality\n",
    "            done |= len(self.unlab_data) <= self.candidate_size\n",
    "            \n",
    "            # safe the experience\n",
    "            self.replay_memory.push((state, action, -1, new_state, done, possible_actions))\n",
    "            \n",
    "            # update state\n",
    "            state = new_state\n",
    "            self.agent.current_state = state\n",
    "            \n",
    "            # sample a batch (if possible) and train the DQN\n",
    "            # TODO: consider the TD-Error when sampling\n",
    "            if self.replay_memory.can_provide(self.batch_size):\n",
    "                self.agent.train(self.replay_memory.sample(self.batch_size))\n",
    "            \n",
    "    \n",
    "    def random_2D_sample(array, size):\n",
    "        \"\"\"\n",
    "        sample a total of 'size' random 1-D arrays out of a 2-D array\n",
    "        \"\"\"\n",
    "        if size > len(array):\n",
    "            raise ValueError(\"Tried to sample more data than existed\")\n",
    "        idx = np.random.randint(len(array), size=size)\n",
    "        return array[idx,:], idx\n",
    "    \n",
    "    def get_X_y(self, data):\n",
    "        \"\"\"\n",
    "        if X and y are united in data, return X and y seperated\n",
    "        \"\"\"\n",
    "        n_features = len(data[0]) - 1\n",
    "        return np.delete(data, n_features, 1), np.delete(data, range(n_features), 1)\n",
    "    \n",
    "    def label_sample(self, idx):\n",
    "        \"\"\"\n",
    "        moves the sample self.unlab_data[idx] to self.lab_data\n",
    "        \"\"\"\n",
    "        sample = self.unlab_data[idx]\n",
    "        self.unlab_data = np.delete(self.unlab_data, idx, 0)\n",
    "        self.lab_data = np.concatenate((self.lab_data, np.expand_dims(sample, axis=0)))\n",
    "        \n",
    "    def split_data(self):\n",
    "        \"\"\"\n",
    "        distributes the data among evaluation, unlabeled and labeled data\n",
    "        \"\"\"\n",
    "        np.random.shuffle(self.all_data)\n",
    "        self.eval_data = all_data[:self.eval_data_ind]\n",
    "        self.unlab_data = all_data[self.eval_data_ind:self.unlab_data_ind]\n",
    "        self.lab_data = all_data[self.unlab_data_ind:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Only for testing purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 3, 5])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([1,2,3,4])\n",
    "b = np.array([True, False, False, True])\n",
    "\n",
    "np.array([a[i]+1 if b[i] else a[i] for i in range(len(a))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array([[2,3,4],\n",
    "              [1,5,3],\n",
    "              [5,3,3],\n",
    "              [5,3,3],\n",
    "              [5,3,4],\n",
    "              [2,7,9],\n",
    "              [5,3,1],\n",
    "              [7,1,4],\n",
    "              [3,2,9],\n",
    "              [2,2,1]])\n",
    "y = np.array([0,0,1,1,0,0,1,1,1,1])\n",
    "pred = np.array([0,1,1,1,0,0,1,1,0,1])\n",
    "\n",
    "5%100\n",
    "\n",
    "# np.concatenate((X, np.expand_dims(np.array([1,1,1]), axis=0)))\n",
    "# test = LAL_RL(X,y,0.5,3,2,None,0.98, lambda x,y:(x-y)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001E66CEE6940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[[-0.5097586 ]\n",
      " [-0.07278678]\n",
      " [-0.1229156 ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-0.07278678"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network = Sequential(\n",
    "            [\n",
    "                layers.Dense(5, input_shape=(3,), activation=\"sigmoid\", name=\"layer1\"),\n",
    "#                 layers.Dense(80, activation=\"sigmoid\", name=\"layer2\"),\n",
    "                layers.Dense(1, name=\"layer3\"),\n",
    "            ]\n",
    "        )\n",
    "# print(network.get_weights())\n",
    "# plot_model(network, show_shapes=True, show_layer_names=True)\n",
    "pred = network.predict(np.array([np.array([1,2,3]),np.array([3,2,1]),np.array([5,2,4])]))\n",
    "print(pred)\n",
    "max([x[0] for x in pred])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
